{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson likelihood calculator\n",
    "\n",
    "Given $M$ components, each with an estimated rate $\\vec{\\beta}$ determined by a normal distribution with uncertainty $\\vec{\\sigma}$, calculate the confidence itervals and perform a hypothesis tests for each parameter $b$.\n",
    "\n",
    "Nominally each event corresponds to a set of observables $\\vec{x}$ of $N$ measurements, for any given measurement, the probability for that particular measurement to come from a particular components is given by\n",
    "\n",
    "$$ P_i(\\vec{x}) \\tag{1}$$\n",
    "\n",
    "The prior probability is then formed through a combination of these components such that the total probability is \n",
    "\n",
    "$$ \\mathbf{P} = \\sum_i^M P_i(\\vec{x}) \\tag{2}$$\n",
    "\n",
    "The likelihood for a full data set of $N$ measurements is the product of each event total probability\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) / \\sum_i^Mb_i \n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "We can extend the likelihood by proclaiming that each components as well as the sum of components are simply a stochastic process, produces the extended likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Finally, we can claim that we have _a priori_ knowledge of the parameters, whether it be through side-band analysis or external constraints, by including those constraints via some prior probability. Given no specific knowledge of the shape of that prior, we will consider the information we receive on the variables to be normally distributed and multiply the likelihood by those constraints\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \n",
    "\\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "\\frac{1}{\\sqrt{2\\pi \\sigma_i^2}}\n",
    "\\text{exp}\\left({\\frac{-(\\beta_i-b_i)^2}{2\\sigma_i^2}}\\right)\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "A few definitions to simplify things:\n",
    "$$ \\lambda := \\sum_i^Mb_i \\tag{6}$$\n",
    "\n",
    "Then then our objective function $\\mathcal{O} = -\\text{Ln}\\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\mathcal{O} = \\lambda + \\text{Ln}N! \n",
    "-\\sum_j^N\\text{Ln}\\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "+ \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} \n",
    "    + \\text{Ln}\\sqrt{2\\pi \\sigma_i} \\right)\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "Finally, for a counting analysis we assume that an optimal set of cuts has been applied which optimizes the sensitivity to a particular parameter, which simplifies the likelihood such that\n",
    "\n",
    "$$ P_i(\\vec{x}) := 1 \\tag{8}$$\n",
    "\n",
    "Also, because the shape of the likelihood space is independent of constant parameters, we can drop the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ terms. We could also remove the $\\text{Ln}N!$ term as well, but for numerical stability we will keep it around, but use Sterling's approximation: $\\text{Ln}N! \\approx N\\text{Ln}N - N$. The remaining objective function we will thus use is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O} = \\lambda - N\\text{Ln}\\lambda + N\\text{Ln}N - N \n",
    "    + \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} \\right)\n",
    "\\tag{9}\n",
    "$$\n",
    "\n",
    "_Note: If the different values of $\\beta$ differ by orders of magnitude, it might be worth forming an affine invariant form of the likelihood, otherwise the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ term should not matter_\n",
    "\n",
    "[Profile Likelihood](https://www.stat.tamu.edu/~suhasini/teaching613/chapter3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"../src/\")\n",
    "using Batman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "Three structures are used to build, test, and fit models. These are the `Model`, `Component`, and `Result`--all of which are mutable.\n",
    "\n",
    "We begin by initializing a default `Model` and one-by-one use `add_component!` to modify the `Model`.\n",
    "\n",
    "The intention is to provide multiple ways to evaluate the uncertainties, the default here is to compute the profile likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CountingExperiment()\n",
    "\n",
    "add_component!(m, \"Signal\", 20.0)\n",
    "add_component!(m, \"Bkg 1\", 30.0; σ=0.1)\n",
    "add_component!(m, \"Bkg 2\", 40.0; σ=45.0)\n",
    "add_component!(m, \"Bkg 3\", 12.0; σ=12.0)\n",
    "\n",
    "set_counts!(m, 100)\n",
    "\n",
    "results = minimize!(m)\n",
    "\n",
    "compute_profiled_uncertainties!(results; σ=1)\n",
    "pretty_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "plt.style.use(\"bat.mplstyle\")\n",
    "\n",
    "hs = x -> x >= 0 ? 1 : 0\n",
    "#profile!(\"Signal\", results; prior=nothing)\n",
    "#uncertainty!(\"Signal\", results )\n",
    "\n",
    "interval_plot(results, \"Signal\")\n",
    "plt.savefig(\"profile.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "correlation_plots(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plots2(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix -- Derive from correlation_plots, store in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake experiments; two set => Positive, Negative\n",
    "\n",
    "## Is this the correct way to express everything. One model per time t (say day)\n",
    "\n",
    "days = collect(1:5:1000) # 1 to 100 days\n",
    "days = exp10.(range(0,stop=4,length=100))\n",
    "hs = x -> x >= 0 ? 1 : 0\n",
    "\n",
    "results_set = []\n",
    "for d in days\n",
    "    m = CountingExperiment()\n",
    "\n",
    "    add_component!(m, \"Signal\", 2.0*d)\n",
    "    add_component!(m, \"Bkg 1\", 3.0*d; σ=0.2*d)\n",
    "\n",
    "    set_counts!(m, 4.0 * d)\n",
    "\n",
    "    results = minimize!(m)\n",
    "    profile!(\"Signal\", results; prior=hs, step=0.1, stop=20)\n",
    "    uncertainty!(\"Signal\", results, σ=1)   \n",
    "    push!(results_set, results)\n",
    "end\n",
    "\n",
    "# Results stored in a DataFrame.jl\n",
    "#compute_profiled_uncertainties!(results; σ=1)\n",
    "#pretty_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SpecialFunctions\n",
    "sens = []\n",
    "\n",
    "function sensitivity(name, results;\n",
    "    mode=\"FC\", prior=nothing)\n",
    "\n",
    "    comp = getparam(results.model, name)\n",
    "    x, y = comp.likelihood_x, comp.likelihood_y\n",
    "    if prior != nothing\n",
    "        y = y .* prior.(x)\n",
    "    end\n",
    "    xselect = (x .>= 0)\n",
    "    x = x[xselect]\n",
    "    y = y[xselect]\n",
    "    y = y / sum(y)\n",
    "    cumulative = cumsum(y)\n",
    "    if mode == \"FC\"\n",
    "        cy = sortperm(y; rev=true)\n",
    "        cum_y = cumsum(y[cy])\n",
    "        cum_x = x[cy]\n",
    "        # Need to find where it first crosses zero\n",
    "        first_min = cum_y[ cum_x .== x[1] ]\n",
    "    end\n",
    "    return minimum([first_min[1], 1])\n",
    "end\n",
    "\n",
    "@show sensitivity(\"Signal\", results_set[20])\n",
    "\n",
    "for rs in results_set\n",
    "    push!(sens, erfinv( sensitivity(\"Signal\", rs))*2^0.5 )\n",
    "end\n",
    "\n",
    "\n",
    "plt.plot(days, 1/sqrt(4)*sqrt.(days),\".\")\n",
    "plt.plot(days, 1/sqrt(3)*sqrt.(days),\".\")\n",
    "plt.plot(days, sens)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "#interval_plot(results_set[20], \"Signal\")\n",
    "#@show days[ lb .>= 0 ][1]\n",
    "#plot(days, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cowen Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor code into sub-files, project.toml, mybinder, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time dependent systematic (maybe)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
